# Yelp-Out-Loud
Businesses are growing at an increasing rate beyond imagination and there needs to be special technologies to handle this excruciating demand. We will implement a Java application that allows customers to connect to a NoSQL database. From there, the customers will be able to retrieve results by querying for specific or general demands. This is basically a Java application that connects the customer to the backend, where executing query functions happen. Then, return the retrieved data to the front end, which will be displayed on the customer’s terminal. These types of databases allow the storage and extraction of various, enormous datasets. In particular, MongoDB will be chosen as our database because it is robust at handling incremental data and data will be configured in a loose schema. Loose schema breaks the conventional, restrictive style of a strict schema by supporting the storage of various data types in one document. A field can be composed of an array of data inside a document and a document can also be stored within another document. Depending on how MongoDB is constructed, space management and query performance can be more efficient than other databases. 

For our dataset, we decided to select Yelp because it provides multiple datasets in the form of JSON files. Yelp datasets, such as business, checkin, review, tip, or user are available. We also chose Yelp, since it is relevant in today’s society. Many businesses or services are competing by marketing themselves on online platforms. Many users utilize Yelp to find businesses or services. For example, searching for the best rated Tacos in the surrounding environment. These contrasting datasets give our team the flexibility to store huge, various data in our application. Having this much Yelp information also gives us a foresight on how a customer might interact with our application. We need to use our insights on producing an application that is customer friendly; the application will ask what the customer wants, translate the demand into a query, and then display results in a professional manner. The last technology we plan on implementing for our application is a cloud service, called Amazon Web Services (AWS). We will install MongoDB on this cloud platform to demonstrate remote access, clustering and the populating of data.  
## Project Successes
We had a lot of successes during our journey of completing this project. Our successes included being able to efficiently organize our meetings, split the workload, and execute given tasks. Discord, a telecommunication service, and Google Docs played big roles on how we achieved our successes. We were able to conduct meetings over Discord and organize our plans or documentations on both Discord and Google Docs. Discord also served as a text platform, so whenever we weren’t voice calling, team members messaged to update their status or progress, and if any concerns, issues, or problems arose. 

Our group didn’t have issues allocating tasks. Each member was assigned tasks based on their skill set and each was responsible for completing it on time. Each member was also willing to help one another if an issue occurred. For example, each member was responsible for wrangling a particular collection’s data and formulating MongoDB queries based on the given use cases. By distributing the workload to each member, we were able to independently test on a small scale. It is important to execute tasks on a small scale production because we need to catch system issues before moving onto an actual, large scale production. An issue would indicate that our current plan is not foolproof. Therefore, adjustments need to be made prior to live production. 

These smaller acts inevitably led to a successful project. We were able to initialize multiple AWS instances and then installed and configured MongoDB on those instances. Importing wrangled JSON data into our MongoDB on AWS was achieved. From there, we constructed a cluster with our appropriate sharding schemes. A successful Python application, which connects the user to our MongoDB, was produced. Our application presents the user a menu of our 15 use cases and requests user input. Next, their input gets converted into a query. A result will return and be displayed to the user.

## Unexpected Events
### Data wrangling after populating data into the sharded cluster
A number of attributes in our dataset were in a format different from what was needed for our use cases. For example, elite in user collection was initially a string while we needed an array of strings. 
We initially tried to wrangle the data after it was loaded to the sharded cluster. This process involved iterating through all the records in the sharded collection and update each one of them. This caused a lot of performance overhead and our machines crashed randomly especially for large collections before the wrangling could be completed. This could be because our machines had minimal hardware resources. To alleviate this problem, we wrangled the data on our local mongodb instances and exported the collections as json files and imported them into the sharded mongodb cluster running on 
### EC2 instances ran out of space
The three EC2 instances hosting our sharded cluster were initially configured with 8GB of disk space. Since the size of our dataset was around 2.80GB we expected that 8GB would be sufficient. However, once all the collections were loaded we eventually ran out of space on all the EC2 instances. We suspect that this could be because of the buildup duplicate shards during rebalancing.
So we increased the disk space by AWS documentation and rebooted the machines. In this process, we did not re-install the mongodb.
### Inefficient Coding Language
Originally, the programming was supposed to be done in Java. Coding had begun for around 5 of the functionalities before we realized that the driver was quite inefficient to create the Mongo commands since it required us to create so many Document objects. There was no data structure in Java that would allow us to easily translate our terminal mongo commands to code. After looking through other drivers, we decided to switch the application code to Python. With Python, it contains very similar data structures similar to JSON such as dictionaries, so it was very easy to translate the mongo commands. By doing so, we were able to save much more time with almost deducting it by 50%.

## Lessons Learned
No matter how well you strategize for a project, there will always be problems. Problems can be unexpected, unforeseen, accepted as a tradeoff, or as a result of bad planning. Our group encountered some issues along the way. During the intermediate phase, we thought we wrangled our data enough. We reduced the amount of documents for a collection’s dataset because its size was too much by default. However, our cluster’s performance was still taxed. We even had our AWS instances equipped with a lot of space, yet failures occurred. These issues were discovered when we were testing several queries. It involved MongoDB queries that take a document’s field and convert its values into an array of strings. The queries would either take an excruciating time to complete, fail instantly, seem to never end, or finally return an error message after some lengthy amount of time. So our solution was to wrangle the data from big collections even more. We also deleted our database and reconfigured our cluster just to be safe. Thus, a lesson here would be that projects go through cycles of trial and error. 

A project can be easier when each member has a similar background to one another. All of us have some experience on how to effectively operate AWS and MongoDB. If one member needs help or is missing in action, other members can chip in. We initially decided that our application will be in Java because everyone is more comfortable with it than other languages. However, comfortability doesn’t breed competition. A person that is comfortable won’t be motivated to learn and innovate new products. We may have intellectual knowledge about AWS and MongoDB, but we are not qualified to call ourselves experienced. We lack actual experience in deploying such technologies. All of us have only deployed a cluster once prior to this project. 

We discovered that converting MongoDB queries into Java was actually difficult or confusing. Another issue is that an object would have to be created every time a query was called. Therefore, our application and database performance can potentially be negatively impacted. Our group researched and found that Python was easier for user inputs and MongoDB queries than Java. Despite the simplicity, this language was practically new for us since we rarely use it. We still had to search documentation on how to properly connect Python to our server and if the queries are implemented correctly. Moral of the story is that we gain experience by doing,  innovate by applying new tools or technologies, and learn by reading documentation.

Since all of us remotely collaborated on this project it was important for us to have a stable mongodb cluster. If we start mongodb as a foreground process, closing the ssh terminal would result in the mongodb process getting stopped. We identified this problem earlier and found out that mongo process can be started as background processes using the fork option. This made our work simple and avoided the need to restart the processes each time a terminal disconnects or the need to have multiple terminals open at the same time.

## Dataset
The data set we have decided to choose is the Yelp dataset which can be downloaded from https://www.yelp.com/dataset/. Previews and documentation can also be examined on the following website: https://www.kaggle.com/yelp-dataset/yelp-dataset.  The dataset was originally for the Yelp Dataset Challenge and contains information about businesses across 8 metropolitan areas in the United States and Canada. The dataset contains five json files: business.json, review.json, user.json, checkin.json, tip.json.  Each json file contains one JSON object per line so it’ll be easy to import to the database system. JSON stands for Javascript Object Notation. There are around 8,635,403 reviews, 160,585 businesses, 1,162,119 tips,  2,189,457 users, and  around 180,000 checkins contained within the dataset so there will be some wrangling needed which will be explained in the next section. Review.json contains different reviews by users about distinct businesses. The business.json contains different businesses in the 8 metropolitan areas and provides distinct attributes about those particular businesses. The tips.json provides different tips created by users about the businesses in the dataset. The users.json provides the different users in the yelp system and their attributes. Lastly, the checkin.json provides information on the checkins for the different businesses.

Due to our large JSON files, we are wrangling data into a smaller memory size or else our database would end up holding 11 GB. Our goal is to build a small cluster with a database under 4 GB. This will make it more efficient for us to store data, load data, and retrieve data. The User and Review collections hold the bulk of the memory storage. In order to reduce the memory size of these collections, we essentially had to choose the amount of documents that we wanted. Then, trim or remove the remaining documents. This was achieved by writing "X" amount of documents from the original json file to a newly created json file. A for loop with a designated counter, which represented the amount of documents we wanted, handled the reading of the original JSON and writing or appending of documents into the new JSON. 

Additional modifications to our data involved changing certain fields into arrays. We modified the Checkin and Business collection. For example, in Checkin collection, there is a field called date, which is in string format. The issue with the original format was that there were multiple dates, separated by commas, under one set of quotes. Some documents' date field have over a hundred dates.

That doesn't sound logical and is a mess if we are to keep that format because there would be no way to specifically call one single date. Therefore, we decided to convert these fields into arrays of string values while separating each existing value with commas.

## Database Schema
We will have one database called yelp which will contain 5 collections: business, checkin, reviews, tisp, and users. Each collection by default will have the _id field as an index as it is implemented by the system. Furthermore we will have different sharding keys and strategies for each one.

The business documents will contain information about the business such as location, business name and amount of stars. The checkin documents will contain the business_id  and the different dates of times of the distinct check in of that specific business. The reviews documents will contain fields such as  id of the review, id of the business, and review description. The tips document will be similar to the review as it contains fields such as the id of the business that the tip is for, and the description of the tip. The users documents will contain information about the user like the user name and review count.

For our shard keys, there were many factors that had to be considered before making our selections. Such factors involved the speed and efficiency of various CRUD operations. We asked these questions from the user perspective, since they are using our application after all. 

The Business collection will be using the postal_code field with range based sharding strategy since users search for specific businesses or services in or nearby their current location. Since it is range based, nearby zip codes would be in the same chunk as each other. This will allow users to quickly search for businesses. The postal_code is better than utilizing a compound key of state and city as it’s much more specific. Having post_code as the shard key for the collection will enable quick queries or reads for the users. This principle of choosing shard keys based on quicker queries or reads for our users applies to our Reviews and Tips collections as well. The Reviews collection will be utilizing range based sharding on the field of business_id. Many users like to read reviews from certain businesses, so this will allow for quick queries of reviews from a certain business.  Furthermore, the Tips collection will also utilize business_id as a shard key for similar reasons to allow for quick queries of tips from certain businesses. Although there may be  downsides like hotspots, range based was chosen for these collections as most users would view these collections, rather than write to them. 

The Checkin and Users collection will differ from other collections because of a different goal. These collections will focus on quicker updates or writes. Thus, appropriate keys must be considered for this task. The Checkin collection will have the field business_id as its shard key  while the User collection will select the field user_id for its shard key. Both collections will utilize range based sharding, but since the fields are pre-hashed, users and chunks will be randomly distributed. For the Checkin collection, the user usually searches for the business_id then  checks into the business by updating the date field. Since users usually check in to businesses many times, this would allow for quicker updates as the updates from several users will be uniform.  For  the Users collection, this is where the user will update their profile. Since the user documents will be randomly distributed, this will allow for multiple user updates to be uniformed.



##
